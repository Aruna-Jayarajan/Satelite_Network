{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aruna\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch_geometric\\typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "c:\\Users\\aruna\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch_geometric\\typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "c:\\Users\\aruna\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch_geometric\\typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(\n",
      "c:\\Users\\aruna\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch_geometric\\typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tensorflow.keras.models import load_model\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "\n",
    "# ========== Add src/ to path if needed ==========\n",
    "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "# ========== Imports from project ==========\n",
    "from dataloader import SatelliteDataset, NUM_GATEWAYS\n",
    "from model2 import Stage2GNN\n",
    "from train2 import train_model_with_coverage\n",
    "from utils import plot_metrics, build_gateway_to_cells_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 8 training files and 2 validation files.\n",
      "✅ Loaded gateway-to-cell mapping with 54 gateways covering 4569 cells.\n",
      "[Epoch 02] Train Loss: 3.8893 | Val Loss: 3.9407 | Top-1: 0.072 / 0.205\n",
      "[Epoch 04] Train Loss: 3.5441 | Val Loss: 3.4962 | Top-1: 0.250 / 0.302\n",
      "[Epoch 06] Train Loss: 3.1871 | Val Loss: 3.1068 | Top-1: 0.349 / 0.340\n",
      "[Epoch 08] Train Loss: 2.8207 | Val Loss: 2.7750 | Top-1: 0.396 / 0.352\n",
      "[Epoch 10] Train Loss: 2.5123 | Val Loss: 2.5626 | Top-1: 0.422 / 0.332\n",
      "[Epoch 12] Train Loss: 2.2901 | Val Loss: 2.4535 | Top-1: 0.440 / 0.375\n",
      "[Epoch 14] Train Loss: 2.1520 | Val Loss: 2.4229 | Top-1: 0.457 / 0.378\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========== Load Dataset ==========\n",
    "DATA_FOLDER = r\"C:\\Users\\aruna\\Desktop\\MS Thesis\\Real Data\\Final folder real data\"\n",
    "\n",
    "file_list = sorted([\n",
    "    os.path.join(DATA_FOLDER, f)\n",
    "    for f in os.listdir(DATA_FOLDER)\n",
    "    if f.endswith('.csv')\n",
    "])[:10]\n",
    "\n",
    "train_size = int(0.8 * len(file_list))\n",
    "train_files, val_files = file_list[:train_size], file_list[train_size:]\n",
    "\n",
    "train_dataset = SatelliteDataset(train_files)\n",
    "val_dataset = SatelliteDataset(val_files)\n",
    "\n",
    "train_loader = PyGDataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = PyGDataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"✅ Loaded {len(train_files)} training files and {len(val_files)} validation files.\")\n",
    "\n",
    "# ========== Load Stage 1 Model ==========\n",
    "STAGE1_MODEL_PATH = 'stage_1_model.h5'\n",
    "stage1_model = load_model(STAGE1_MODEL_PATH)\n",
    "\n",
    "# ========== Build Gateway-to-Cell Mapping ==========\n",
    "GATEWAY_CELL_CSV = r\"C:\\Users\\aruna\\Desktop\\MS Thesis\\Real Data\\cells_with_gateways.csv\"\n",
    "gw_to_cells = build_gateway_to_cells_mapping(GATEWAY_CELL_CSV)\n",
    "# Count unique cell indices across all gateways\n",
    "total_cells = len(set(cell for cells in gw_to_cells.values() for cell in cells))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"✅ Loaded gateway-to-cell mapping with {len(gw_to_cells)} gateways covering {total_cells} cells.\")\n",
    "\n",
    "# ========== Build GNN Model ==========\n",
    "input_dim = 3 + NUM_GATEWAYS * 3  # position + visibility + top3 gateways + neighbors\n",
    "gnn_model = Stage2GNN(\n",
    "    input_dim=input_dim,\n",
    "    sat_feature_dim=111,\n",
    "    neighbor_feature_dim=NUM_GATEWAYS,\n",
    "    hidden_dim=256,\n",
    "    output_dim=NUM_GATEWAYS,\n",
    "    dropout=0.3,\n",
    "    use_residual=True\n",
    ")\n",
    "\n",
    "optimizer_gnn = optim.Adam(gnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# ========== Train with Coverage-Aware Loss ==========\n",
    "results = train_model_with_coverage(\n",
    "    gnn_model=gnn_model,\n",
    "    stage1_model=stage1_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer_gnn=optimizer_gnn,\n",
    "    gw_to_cells=gw_to_cells,              # NOTE: updated argument\n",
    "    total_cells=total_cells,\n",
    "    num_epochs=20,\n",
    "    rounds=15,\n",
    "    lambda_global=0.2,\n",
    "    lambda_entropy=0.01,\n",
    "    lambda_coverage=1.0,\n",
    "    label_smoothing=0.1\n",
    ")\n",
    "\n",
    "(train_losses, val_losses,\n",
    " train_top1_acc, train_top3_acc, train_top5_acc,\n",
    " val_top1_acc, val_top3_acc, val_top5_acc) = results\n",
    "\n",
    "# ========== Plot ==========\n",
    "plot_metrics(train_losses, val_losses,\n",
    "             train_top1_acc, train_top3_acc, train_top5_acc,\n",
    "             val_top1_acc, val_top3_acc, val_top5_acc)\n",
    "\n",
    "# ========== Save ==========\n",
    "torch.save(gnn_model.state_dict(), 'stage2_loop_gnn_model_with_coverage.pth')\n",
    "print(\"✅ GNN model saved with coverage-aware training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluating Train Range 0-5 (Files 0 to 5) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aruna\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 0.8109\n",
      "Top-3 Accuracy: 0.9896\n",
      "Top-5 Accuracy: 0.9986\n",
      "Saved predictions to test_results_Train_Range_0_5_coverage.csv\n",
      "\n",
      "===== Evaluating Train Range 5-10 (Files 5 to 10) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aruna\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 0.6190\n",
      "Top-3 Accuracy: 0.8705\n",
      "Top-5 Accuracy: 0.9064\n",
      "Saved predictions to test_results_Train_Range_5_10_coverage.csv\n",
      "\n",
      "===== Evaluating Test Range 100-105 (Files 100 to 105) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aruna\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 0.4001\n",
      "Top-3 Accuracy: 0.7244\n",
      "Top-5 Accuracy: 0.8312\n",
      "Saved predictions to test_results_Test_Range_100_105_coverage.csv\n",
      "\n",
      "===== Evaluating Test Range 200-205 (Files 200 to 205) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aruna\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 0.3516\n",
      "Top-3 Accuracy: 0.6650\n",
      "Top-5 Accuracy: 0.7626\n",
      "Saved predictions to test_results_Test_Range_200_205_coverage.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.data import DataLoader\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from dataloader import prepare_input_for_gnn, build_graph_from_file\n",
    "from utils import top_k_accuracy\n",
    "from model2 import Stage2GNN  # use model2 for coverage-aware model\n",
    "\n",
    "# Constants\n",
    "NUM_GATEWAYS = 54  # Define this according to your setup\n",
    "\n",
    "# Paths\n",
    "DATA_FOLDER = r\"C:\\Users\\aruna\\Desktop\\MS Thesis\\Real Data\\Final folder real data\"\n",
    "STAGE1_MODEL_PATH = 'stage_1_visible_model.h5'\n",
    "STAGE2_MODEL_PATH = 'stage2_loop_gnn_model_with_coverage.pth'\n",
    "\n",
    "# Load Stage 1 model\n",
    "stage1_model = load_model(STAGE1_MODEL_PATH)\n",
    "\n",
    "# Initialize and load the GNN model\n",
    "input_dim = 3 + NUM_GATEWAYS * 3\n",
    "gnn_model = Stage2GNN(\n",
    "    input_dim=input_dim,\n",
    "    sat_feature_dim=111,\n",
    "    neighbor_feature_dim=NUM_GATEWAYS,\n",
    "    hidden_dim=256,\n",
    "    output_dim=NUM_GATEWAYS,\n",
    "    dropout=0.3,\n",
    "    use_residual=True\n",
    ")\n",
    "gnn_model.load_state_dict(torch.load(STAGE2_MODEL_PATH))\n",
    "gnn_model.eval()\n",
    "\n",
    "# List all .csv files\n",
    "file_list = sorted([\n",
    "    os.path.join(DATA_FOLDER, f)\n",
    "    for f in os.listdir(DATA_FOLDER)\n",
    "    if f.endswith('.csv')\n",
    "])\n",
    "\n",
    "# Define test ranges\n",
    "test_ranges = {\n",
    "    \"Train Range 0-5\": (0, 5),\n",
    "    \"Train Range 5-10\": (5, 10),\n",
    "    \"Test Range 100-105\": (100, 105),\n",
    "    \"Test Range 200-205\": (200, 205)\n",
    "}\n",
    "\n",
    "# Function to evaluate test datasets\n",
    "def evaluate_test_set(name, start, end):\n",
    "    print(f\"\\n===== Evaluating {name} (Files {start} to {end}) =====\")\n",
    "    test_files = file_list[start:end]\n",
    "    test_graphs = [build_graph_from_file(f) for f in test_files if build_graph_from_file(f) is not None]\n",
    "    test_loader = DataLoader(test_graphs, batch_size=1, shuffle=False)\n",
    "\n",
    "    total_top1, total_top3, total_top5, total_samples = 0, 0, 0, 0\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            input_features = data.x[:, :57].cpu().numpy()\n",
    "            stage1_preds = stage1_model.predict(input_features, verbose=0)\n",
    "            top3_model1 = np.argsort(stage1_preds, axis=1)[:, -3:]\n",
    "\n",
    "            binary_preds_model1 = np.zeros_like(stage1_preds)\n",
    "            for i, idx in enumerate(top3_model1):\n",
    "                binary_preds_model1[i, idx] = 1\n",
    "\n",
    "            gnn_input = prepare_input_for_gnn(data, torch.tensor(binary_preds_model1, dtype=torch.float))\n",
    "            preds = gnn_model(gnn_input.x, gnn_input.edge_index)\n",
    "\n",
    "            total_top1 += top_k_accuracy(preds, data.y, k=1)\n",
    "            total_top3 += top_k_accuracy(preds, data.y, k=3)\n",
    "            total_top5 += top_k_accuracy(preds, data.y, k=5)\n",
    "            total_samples += 1\n",
    "\n",
    "            top1_pred = torch.topk(preds, k=1, dim=1).indices.squeeze().tolist()\n",
    "            if isinstance(top1_pred, int):  # handle single-sample batch\n",
    "                top1_pred = [top1_pred]\n",
    "\n",
    "            for i in range(len(data.y)):\n",
    "                predictions.append([data.y[i].item(), top1_pred[i]])\n",
    "\n",
    "    print(f\"Top-1 Accuracy: {total_top1 / total_samples:.4f}\")\n",
    "    print(f\"Top-3 Accuracy: {total_top3 / total_samples:.4f}\")\n",
    "    print(f\"Top-5 Accuracy: {total_top5 / total_samples:.4f}\")\n",
    "\n",
    "    results_df = pd.DataFrame(predictions, columns=[\"Ground Truth\", \"Top-1 Prediction\"])\n",
    "    filename = f\"test_results_{name.replace(' ', '_').replace('-', '_')}_coverage.csv\"\n",
    "    results_df.to_csv(filename, index=False)\n",
    "    print(f\"Saved predictions to {filename}\")\n",
    "\n",
    "# Run evaluation for each test range\n",
    "for name, (start, end) in test_ranges.items():\n",
    "    evaluate_test_set(name, start, end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "c:\\Users\\aruna\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary ===\n",
      "Unique Satellites: 112\n",
      "Satellites With Assigned Cells: 84 (75.00%)\n",
      "Satellites With No Assigned Cells: 28 (25.00%)\n",
      "Gateways With Unassigned Cells: 0 (0.00%)\n",
      "Total Gateways: 54\n",
      "Used Gateways: 54 (100.00%)\n",
      "Unused Gateways: []\n",
      "Assigned Cells: 4569 / 4569 (100.00%)\n",
      "Unassigned Cells: 0 (0.00%)\n",
      "\n",
      "=== Output File Preview ===\n",
      "   unique_satellite_id  predicted_gateway  actual_gateway_id  \\\n",
      "0                    5                  2                  2   \n",
      "1                    7                 34                 34   \n",
      "2                    8                 29                 29   \n",
      "3                   28                  3                  2   \n",
      "4                   29                 21                 21   \n",
      "\n",
      "                                      assigned_cells  \\\n",
      "0                                                 []   \n",
      "1                                                 []   \n",
      "2  [92, 108, 163, 212, 303, 375, 440, 536, 598, 7...   \n",
      "3              [144, 347, 483, 611, 848, 1081, 1139]   \n",
      "4  [1455, 1469, 1477, 1489, 1581, 1582, 1608, 172...   \n",
      "\n",
      "                                     actual_cell_ids  \n",
      "0  [87, 912, 1021, 1137, 1154, 1158, 1315, 1338, ...  \n",
      "1  [176, 274, 312, 345, 399, 416, 445, 512, 586, ...  \n",
      "2  [92, 108, 148, 163, 212, 303, 375, 536, 598, 7...  \n",
      "3  [144, 347, 483, 730, 1081, 1192, 1317, 1428, 1...  \n",
      "4  [14, 86, 111, 231, 273, 302, 323, 348, 446, 55...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "from tensorflow.keras.models import load_model\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from dataloader import build_graph_from_file, prepare_input_for_gnn, NUM_GATEWAYS\n",
    "from model import Stage2GNN\n",
    "\n",
    "# === Paths ===\n",
    "cells_path = r\"C:\\Users\\aruna\\Desktop\\MS Thesis\\Real Data\\cells_with_gateways.csv\"\n",
    "data_file = r\"C:\\Users\\aruna\\Desktop\\MS Thesis\\Real Data\\Final folder real data\\file_data_00_00_00.csv\"\n",
    "STAGE1_MODEL_PATH = 'stage_1_model.h5'\n",
    "STAGE2_MODEL_PATH = 'stage2_loop_gnn_model_with_coverage.pth'\n",
    "\n",
    "# === Load Data ===\n",
    "cells_df = pd.read_csv(cells_path)\n",
    "original_df = pd.read_csv(data_file)\n",
    "\n",
    "stage1_model = load_model(STAGE1_MODEL_PATH)\n",
    "\n",
    "gnn_model = Stage2GNN(\n",
    "    input_dim=3 + NUM_GATEWAYS * 3,\n",
    "    sat_feature_dim=111,\n",
    "    neighbor_feature_dim=NUM_GATEWAYS,\n",
    "    hidden_dim=256,\n",
    "    output_dim=NUM_GATEWAYS\n",
    ")\n",
    "gnn_model.load_state_dict(torch.load(STAGE2_MODEL_PATH))\n",
    "gnn_model.eval()\n",
    "\n",
    "# === Prepare Graph ===\n",
    "graph = build_graph_from_file(data_file)\n",
    "data_loader = DataLoader([graph], batch_size=1)\n",
    "\n",
    "# === Setup ===\n",
    "unique_sats = original_df['feed_sat'].drop_duplicates().reset_index(drop=True)\n",
    "feed_sat_to_id = {sat: idx for idx, sat in enumerate(unique_sats)}\n",
    "sat_to_indices = defaultdict(list)\n",
    "for row_idx, feed_sat in enumerate(original_df[\"feed_sat\"]):\n",
    "    sat_to_indices[feed_sat].append(row_idx)\n",
    "\n",
    "satellite_to_gateway = {}\n",
    "gateway_to_sats = defaultdict(list)\n",
    "cell_to_gateways = {}  # index → [closest, second]\n",
    "for idx, row in cells_df.iterrows():\n",
    "    cell_to_gateways[idx] = [row[\"closest_gw_id\"], row[\"second_closest_gw_id\"]]\n",
    "\n",
    "# === Model Inference ===\n",
    "top1_preds = []\n",
    "with torch.no_grad():\n",
    "    for data in data_loader:\n",
    "        input_features = data.x[:, :57].cpu().numpy()\n",
    "        stage1_preds = stage1_model.predict(input_features, verbose=0)\n",
    "        top3_indices = np.argsort(stage1_preds, axis=1)[:, -3:]\n",
    "\n",
    "        binary_preds = np.zeros_like(stage1_preds)\n",
    "        for i, idx in enumerate(top3_indices):\n",
    "            binary_preds[i, idx] = 1\n",
    "\n",
    "        gnn_input = prepare_input_for_gnn(data, torch.from_numpy(binary_preds).float())\n",
    "        gnn_output = gnn_model(gnn_input.x, gnn_input.edge_index)\n",
    "        top1_preds = torch.argmax(gnn_output, dim=1).cpu().numpy()\n",
    "\n",
    "        for row_idx, gateway in enumerate(top1_preds):\n",
    "            feed_sat = original_df.iloc[row_idx][\"feed_sat\"]\n",
    "            satellite_to_gateway[feed_sat] = gateway\n",
    "            gateway_to_sats[gateway].append(feed_sat)\n",
    "\n",
    "# === Fair Cell Assignment (Greedy) ===\n",
    "satellite_to_cells = defaultdict(list)\n",
    "assigned_cells = set()\n",
    "used_gateways = set()\n",
    "\n",
    "for gw, sats in gateway_to_sats.items():\n",
    "    nearby_cells = cells_df[\n",
    "        (cells_df[\"closest_gw_id\"] == gw) | (cells_df[\"second_closest_gw_id\"] == gw)\n",
    "    ].index.tolist()\n",
    "\n",
    "    if not sats or not nearby_cells:\n",
    "        continue\n",
    "\n",
    "    used_gateways.add(gw)\n",
    "    sats = list(set(sats))  # Remove dups\n",
    "    n_sats = len(sats)\n",
    "    cell_chunks = np.array_split(nearby_cells, n_sats)\n",
    "\n",
    "    for sat, chunk in zip(sats, cell_chunks):\n",
    "        for cell in chunk:\n",
    "            if cell not in assigned_cells:\n",
    "                satellite_to_cells[sat].append(cell)\n",
    "                assigned_cells.add(cell)\n",
    "\n",
    "# === Summary ===\n",
    "total_unique_sats = len(unique_sats)\n",
    "total_cells = len(cells_df)\n",
    "all_gateways = set(range(NUM_GATEWAYS))\n",
    "\n",
    "assigned_sat_ids = set(satellite_to_cells.keys())\n",
    "unassigned_sat_ids = sorted(set(feed_sat_to_id.keys()) - assigned_sat_ids)\n",
    "unused_gateways = sorted(all_gateways - used_gateways)\n",
    "unassigned_cells = sorted(set(cells_df.index) - assigned_cells)\n",
    "\n",
    "# === Summary ===\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Unique Satellites: {total_unique_sats}\")\n",
    "print(f\"Satellites With Assigned Cells: {len(assigned_sat_ids)} ({len(assigned_sat_ids) / total_unique_sats * 100:.2f}%)\")\n",
    "print(f\"Satellites With No Assigned Cells: {len(unassigned_sat_ids)} ({len(unassigned_sat_ids) / total_unique_sats * 100:.2f}%)\")\n",
    "\n",
    "print(f\"Gateways With Unassigned Cells: {len(unused_gateways)} ({len(unused_gateways) / NUM_GATEWAYS * 100:.2f}%)\")\n",
    "print(f\"Total Gateways: {NUM_GATEWAYS}\")\n",
    "print(f\"Used Gateways: {len(used_gateways)} ({len(used_gateways) / NUM_GATEWAYS * 100:.2f}%)\")\n",
    "print(f\"Unused Gateways: {unused_gateways}\")\n",
    "\n",
    "print(f\"Assigned Cells: {len(assigned_cells)} / {total_cells} ({len(assigned_cells) / total_cells * 100:.2f}%)\")\n",
    "print(f\"Unassigned Cells: {len(unassigned_cells)} ({len(unassigned_cells) / total_cells * 100:.2f}%)\")\n",
    "\n",
    "# === Export Results ===\n",
    "# === Export Results ===\n",
    "output_rows = []\n",
    "all_sat_ids = sorted(feed_sat_to_id.keys())  # Include all satellites, not just assigned ones\n",
    "\n",
    "for sat_feed_val in all_sat_ids:\n",
    "    row_indices = sat_to_indices.get(sat_feed_val, [])\n",
    "    predicted_gateways = [top1_preds[i] for i in row_indices if i < len(top1_preds)]\n",
    "    most_common_gateway = Counter(predicted_gateways).most_common(1)[0][0] if predicted_gateways else None\n",
    "    assigned = satellite_to_cells.get(sat_feed_val, [])\n",
    "\n",
    "    # Convert np.int64 values to int\n",
    "    assigned_clean = [int(cell) for cell in assigned]\n",
    "\n",
    "    actual_gateway = original_df.loc[original_df[\"feed_sat\"] == sat_feed_val, \"gw\"].iloc[0]\n",
    "    actual_cell_ids = original_df.loc[original_df[\"feed_sat\"] == sat_feed_val, \"cell_id\"].tolist()\n",
    "\n",
    "    output_rows.append({\n",
    "        \"unique_satellite_id\": sat_feed_val,\n",
    "        \"predicted_gateway\": most_common_gateway,\n",
    "        \"actual_gateway_id\": actual_gateway,\n",
    "        \"assigned_cells\": assigned_clean,\n",
    "        \"actual_cell_ids\": actual_cell_ids\n",
    "    })\n",
    "\n",
    "output_df = pd.DataFrame(output_rows)\n",
    "output_df.to_csv(\"satellite_to_cells_mapping.csv\", index=False)\n",
    "\n",
    "# === Output preview ===\n",
    "print(\"\\n=== Output File Preview ===\")\n",
    "print(output_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with cells "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "c:\\Users\\aruna\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\aruna\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\aruna\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed inference, visualization, and saved outputs for selected files (model2).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import defaultdict, Counter\n",
    "from tensorflow.keras.models import load_model\n",
    "from torch_geometric.data import DataLoader\n",
    "from dataloader import build_graph_from_file, prepare_input_for_gnn, NUM_GATEWAYS\n",
    "from model2 import Stage2GNN  # Use model2 for coverage-aware model\n",
    "\n",
    "# === Setup Paths ===\n",
    "DATA_FOLDER = r\"C:\\Users\\aruna\\Desktop\\MS Thesis\\Real Data\\Final folder real data\"\n",
    "CELLS_PATH = r\"C:\\Users\\aruna\\Desktop\\MS Thesis\\Real Data\\cells_with_gateways.csv\"\n",
    "GW_PATH = r\"C:\\Users\\aruna\\Desktop\\MS Thesis\\Real Data\\df_gw.csv\"\n",
    "STAGE1_MODEL_PATH = 'stage_1_model.h5'\n",
    "STAGE2_MODEL_PATH = 'stage2_loop_gnn_model_with_coverage.pth'\n",
    "\n",
    "# === Load Models ===\n",
    "stage1_model = load_model(STAGE1_MODEL_PATH)\n",
    "gnn_model = Stage2GNN(\n",
    "    input_dim=3 + NUM_GATEWAYS * 3,\n",
    "    sat_feature_dim=111,\n",
    "    neighbor_feature_dim=NUM_GATEWAYS,\n",
    "    hidden_dim=256,\n",
    "    output_dim=NUM_GATEWAYS,\n",
    "    dropout=0.3,\n",
    "    use_residual=True\n",
    ")\n",
    "gnn_model.load_state_dict(torch.load(STAGE2_MODEL_PATH))\n",
    "gnn_model.eval()\n",
    "\n",
    "# === Prepare Output Directory ===\n",
    "os.makedirs(\"results_with_cells_model2\", exist_ok=True)\n",
    "\n",
    "# === Select Files: 1st, 5th, 100th ===\n",
    "file_list = sorted([f for f in os.listdir(DATA_FOLDER) if f.endswith('.csv')])\n",
    "selected_indices = [0, 4, 99]\n",
    "selected_files = [os.path.join(DATA_FOLDER, file_list[i]) for i in selected_indices if i < len(file_list)]\n",
    "\n",
    "summary_stats = []\n",
    "cells_df = pd.read_csv(CELLS_PATH)\n",
    "gw_df = pd.read_csv(GW_PATH)\n",
    "\n",
    "# === Colors for plotting ===\n",
    "base_colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(base_colors)\n",
    "gateway_color_map = {i: base_colors[i % len(base_colors)] for i in range(NUM_GATEWAYS)}\n",
    "\n",
    "def lighten_color(color, amount=0.5):\n",
    "    import colorsys\n",
    "    try:\n",
    "        c = mcolors.cnames[color]\n",
    "    except:\n",
    "        c = color\n",
    "    r, g, b = mcolors.to_rgb(c)\n",
    "    h, l, s = colorsys.rgb_to_hls(r, g, b)\n",
    "    r, g, b = colorsys.hls_to_rgb(h, min(1, l + amount * (1 - l)), s)\n",
    "    return r, g, b\n",
    "\n",
    "# === Main Loop for Each File ===\n",
    "for file_path in selected_files:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    original_df = pd.read_csv(file_path)\n",
    "    graph = build_graph_from_file(file_path)\n",
    "    data_loader = DataLoader([graph], batch_size=1)\n",
    "\n",
    "    unique_sats = original_df['feed_sat'].drop_duplicates().reset_index(drop=True)\n",
    "    feed_sat_to_id = {sat: idx for idx, sat in enumerate(unique_sats)}\n",
    "    sat_to_indices = defaultdict(list)\n",
    "    for row_idx, feed_sat in enumerate(original_df[\"feed_sat\"]):\n",
    "        sat_to_indices[feed_sat].append(row_idx)\n",
    "\n",
    "    satellite_to_gateway = {}\n",
    "    gateway_to_sats = defaultdict(list)\n",
    "    cell_to_gateways = {idx: [row[\"closest_gw_id\"], row[\"second_closest_gw_id\"]] for idx, row in cells_df.iterrows()}\n",
    "\n",
    "    # === Inference ===\n",
    "    top1_preds = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            input_features = data.x[:, :57].cpu().numpy()\n",
    "            stage1_preds = stage1_model.predict(input_features, verbose=0)\n",
    "            top3_indices = np.argsort(stage1_preds, axis=1)[:, -3:]\n",
    "            binary_preds = np.zeros_like(stage1_preds)\n",
    "            for i, idx in enumerate(top3_indices):\n",
    "                binary_preds[i, idx] = 1\n",
    "\n",
    "            gnn_input = prepare_input_for_gnn(data, torch.from_numpy(binary_preds).float())\n",
    "            gnn_output = gnn_model(gnn_input.x, gnn_input.edge_index)\n",
    "            top1_preds = torch.argmax(gnn_output, dim=1).cpu().numpy()\n",
    "\n",
    "            for row_idx, gateway in enumerate(top1_preds):\n",
    "                feed_sat = original_df.iloc[row_idx][\"feed_sat\"]\n",
    "                satellite_to_gateway[feed_sat] = gateway\n",
    "                gateway_to_sats[gateway].append(feed_sat)\n",
    "\n",
    "    # === Cell Assignment ===\n",
    "    satellite_to_cells = defaultdict(list)\n",
    "    assigned_cells = set()\n",
    "    used_gateways = set()\n",
    "\n",
    "    for gw, sats in gateway_to_sats.items():\n",
    "        nearby_cells = cells_df[(cells_df[\"closest_gw_id\"] == gw) | (cells_df[\"second_closest_gw_id\"] == gw)].index.tolist()\n",
    "        if not sats or not nearby_cells:\n",
    "            continue\n",
    "        used_gateways.add(gw)\n",
    "        sats = list(set(sats))\n",
    "        cell_chunks = np.array_split(nearby_cells, len(sats))\n",
    "        for sat, chunk in zip(sats, cell_chunks):\n",
    "            for cell in chunk:\n",
    "                if cell not in assigned_cells:\n",
    "                    satellite_to_cells[sat].append(cell)\n",
    "                    assigned_cells.add(cell)\n",
    "\n",
    "    # === Save Mapping ===\n",
    "    output_rows = []\n",
    "    all_sat_ids = sorted(feed_sat_to_id.keys())\n",
    "    for sat_feed_val in all_sat_ids:\n",
    "        row_indices = sat_to_indices.get(sat_feed_val, [])\n",
    "        predicted_gateways = [top1_preds[i] for i in row_indices if i < len(top1_preds)]\n",
    "        most_common_gateway = Counter(predicted_gateways).most_common(1)[0][0] if predicted_gateways else None\n",
    "        assigned = satellite_to_cells.get(sat_feed_val, [])\n",
    "        assigned_clean = [int(cell) for cell in assigned]\n",
    "        actual_gateway = original_df.loc[original_df[\"feed_sat\"] == sat_feed_val, \"gw\"].iloc[0]\n",
    "        actual_cell_ids = original_df.loc[original_df[\"feed_sat\"] == sat_feed_val, \"cell_id\"].tolist()\n",
    "        output_rows.append({\n",
    "            \"unique_satellite_id\": sat_feed_val,\n",
    "            \"predicted_gateway\": most_common_gateway,\n",
    "            \"actual_gateway_id\": actual_gateway,\n",
    "            \"assigned_cells\": assigned_clean,\n",
    "            \"actual_cell_ids\": actual_cell_ids\n",
    "        })\n",
    "\n",
    "    mapping_df = pd.DataFrame(output_rows)\n",
    "    mapping_path = f\"results_with_cells_model2/mapping_{file_name.replace('.csv', '')}.csv\"\n",
    "    mapping_df.to_csv(mapping_path, index=False)\n",
    "\n",
    "    # === Summary Stats ===\n",
    "    summary_stats.append({\n",
    "        \"File Name\": file_name,\n",
    "        \"Rows\": len(original_df),\n",
    "        \"Columns\": original_df.shape[1],\n",
    "        \"Unique Satellites\": original_df[\"feed_sat\"].nunique(),\n",
    "        \"Unique Gateways\": original_df[\"gw\"].nunique(),\n",
    "        \"Unique Cells\": original_df[\"cell_id\"].nunique(),\n",
    "        \"Assigned Cells\": len(assigned_cells),\n",
    "        \"Unassigned Cells\": len(cells_df) - len(assigned_cells),\n",
    "        \"Used Gateways\": len(used_gateways),\n",
    "        \"Unused Gateways\": NUM_GATEWAYS - len(used_gateways)\n",
    "    })\n",
    "\n",
    "    # === Visualization ===\n",
    "    def plot_map(title, save_name, include_sats=True, show_lines=False):\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        for _, row in gw_df.iterrows():\n",
    "            plt.scatter(row['longitude'], row['latitude'], marker='^',\n",
    "                        color=gateway_color_map.get(row['gw_id'], 'gray'), edgecolor='black', s=150)\n",
    "\n",
    "        for _, row in mapping_df.iterrows():\n",
    "            pred_gw = row['predicted_gateway']\n",
    "            color = lighten_color(gateway_color_map.get(pred_gw, 'gray'), amount=0.4)\n",
    "            if include_sats:\n",
    "                sat_row = original_df[original_df['feed_sat'] == row['unique_satellite_id']].drop_duplicates('feed_sat')\n",
    "                if not sat_row.empty:\n",
    "                    lng, lat = sat_row.iloc[0][['Longitude', 'Latitude']]\n",
    "                    plt.scatter(lng, lat, marker='s', c=[color], edgecolor='black', s=80)\n",
    "\n",
    "            for cell_idx in row['assigned_cells']:\n",
    "                lng, lat = cells_df.loc[cell_idx, ['lng', 'lat']]\n",
    "                plt.scatter(lng, lat, c=[color], s=10, alpha=0.7)\n",
    "\n",
    "            if show_lines:\n",
    "                pred_row = gw_df[gw_df['gw_id'] == row['predicted_gateway']]\n",
    "                act_row = gw_df[gw_df['gw_id'] == row['actual_gateway_id']]\n",
    "                if not pred_row.empty and not act_row.empty:\n",
    "                    pred_lng, pred_lat = pred_row.iloc[0][['longitude', 'latitude']]\n",
    "                    act_lng, act_lat = act_row.iloc[0][['longitude', 'latitude']]\n",
    "                    plt.plot([pred_lng, act_lng], [pred_lat, act_lat], linestyle='--', color='gray')\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Longitude\")\n",
    "        plt.ylabel(\"Latitude\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"results_with_cells_model2/{save_name}.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    plot_map(\"Predicted Satellites and Gateways\", f\"map1_{file_name.replace('.csv', '')}\")\n",
    "    plot_map(\"Predicted Gateways and Assigned Cells\", f\"map2_{file_name.replace('.csv', '')}\", include_sats=False)\n",
    "    plot_map(\"Predicted vs Actual Gateways\", f\"map3_{file_name.replace('.csv', '')}\", include_sats=False, show_lines=True)\n",
    "\n",
    "# === Save All Summary ===\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "summary_df.to_csv(\"results_with_cells_model2/summary_stats.csv\", index=False)\n",
    "print(\"✅ Completed inference, visualization, and saved outputs for selected files (model2).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
