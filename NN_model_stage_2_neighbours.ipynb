{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import ast  \n",
    "\n",
    "# Load Model 1\n",
    "model_1 = load_model('stage_1_model.h5')\n",
    "\n",
    "# Function to generate filenames\n",
    "def generate_time_file_names(start_time_str, end_time_str, folder_path, prefix='optimal_file_data_'):\n",
    "    start_time = datetime.strptime(start_time_str, '%H_%M_%S')\n",
    "    end_time = datetime.strptime(end_time_str, '%H_%M_%S')\n",
    "    current_time = start_time\n",
    "    while current_time <= end_time:\n",
    "        time_str = current_time.strftime('%H_%M_%S')\n",
    "        file_name = f\"{prefix}{time_str}.csv\"\n",
    "        yield os.path.join(folder_path, file_name)\n",
    "        current_time += timedelta(seconds=20)\n",
    "\n",
    "# Function to process each file\n",
    "def process_file(file_name):\n",
    "    if not os.path.exists(file_name):\n",
    "        return None, None, None, None\n",
    "\n",
    "    optimal_data = pd.read_csv(file_name, usecols=['feed_sat', 'Latitude', 'Longitude', 'Altitude', 'optimal_gateway_matrix'])\n",
    "    optimal_data.drop_duplicates(subset=['feed_sat'], inplace=True)\n",
    "\n",
    "    if optimal_data.empty:\n",
    "        return None, None, None, None  # Avoid processing empty files\n",
    "\n",
    "    # Ensure all values are numeric\n",
    "    optimal_data[['Latitude', 'Longitude', 'Altitude']] = optimal_data[['Latitude', 'Longitude', 'Altitude']].astype(float)\n",
    "\n",
    "    # Extract satellite positions\n",
    "    sat_positions = {row['feed_sat']: row[['Latitude', 'Longitude', 'Altitude']].values for _, row in optimal_data.iterrows()}\n",
    "\n",
    "    # Convert labels from string to array\n",
    "    sat_labels = {row['feed_sat']: np.array(ast.literal_eval(row['optimal_gateway_matrix']), dtype=np.float32) \n",
    "                  for _, row in optimal_data.iterrows()}\n",
    "    \n",
    "    sat_list = list(sat_positions.keys())\n",
    "\n",
    "    if not sat_positions:\n",
    "        return None, None, None, None  # Skip empty datasets\n",
    "\n",
    "    # Convert positions to NumPy array\n",
    "    X_input = np.array(list(sat_positions.values()), dtype=np.float32)\n",
    "\n",
    "    # Ensure valid input\n",
    "    if X_input.ndim != 2 or X_input.shape[1] != 3:\n",
    "        print(f\"Invalid shape for X_input: {X_input.shape} in {file_name}\")\n",
    "        return None, None, None, None  # Skip malformed data\n",
    "\n",
    "    # Get top-3 predicted gateways\n",
    "    top_3_preds = get_top_3_predictions(model_1, X_input)\n",
    "\n",
    "\n",
    "    # Map predictions back to satellites\n",
    "    sat_top_3_map = {sat: pred for sat, pred in zip(sat_list, top_3_preds)}\n",
    "\n",
    "    # Find neighbors only from available satellites\n",
    "    sat_neighbors = {}\n",
    "    for sat, pos in sat_positions.items():\n",
    "        distances = [(other_sat, np.linalg.norm(pos - sat_positions[other_sat])) \n",
    "                     for other_sat in sat_list if other_sat != sat]\n",
    "        nearest_neighbors = sorted(distances, key=lambda x: x[1])[:4]  # Top-4 closest\n",
    "        sat_neighbors[sat] = [sat_positions[n[0]] for n in nearest_neighbors]\n",
    "\n",
    "    return sat_positions, sat_neighbors, sat_top_3_map, sat_labels\n",
    "\n",
    "# Function to get top-3 predictions\n",
    "def get_top_3_predictions(model, X):\n",
    "    predictions = model.predict(X, batch_size=256, verbose=0)\n",
    "    top_3_indices = np.argsort(predictions, axis=1)[:, -3:]\n",
    "    top_3_probs = np.take_along_axis(predictions, top_3_indices, axis=1)\n",
    "    return np.hstack((top_3_indices, top_3_probs))\n",
    "\n",
    "# Training settings\n",
    "folder_path = r\"C:\\Users\\aruna\\Desktop\\MS Thesis\\Real Data\\Files with position\"\n",
    "train_files = list(generate_time_file_names('16_00_00', '19_00_00', folder_path))\n",
    "# Initialize MinMaxScaler before using it\n",
    "scaler = MinMaxScaler()  \n",
    "\n",
    "X_train, y_train = [], []\n",
    "for file_name in train_files:\n",
    "    sat_positions, sat_neighbors, sat_top_3_map, sat_labels = process_file(file_name)\n",
    "    if sat_positions is None:\n",
    "        continue  \n",
    "\n",
    "    for sat in sat_positions.keys():\n",
    "        sat_pos = sat_positions[sat]\n",
    "        neighbor_pos = sat_neighbors[sat]\n",
    "        top_3_preds = sat_top_3_map[sat]\n",
    "        y_label = sat_labels[sat]\n",
    "\n",
    "        feature_vector = np.concatenate([sat_pos] + neighbor_pos + [top_3_preds])\n",
    "\n",
    "        X_train.append(feature_vector)\n",
    "        y_train.append(y_label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.vstack(y_train)\n",
    "\n",
    "# ✅ **Ensure Scaler is Fitted on Full 21 Features**\n",
    "assert X_train.shape[1] == 21, \"Training data should have 21 features!\"\n",
    "scaler.fit(X_train)  # Fit only after full training feature vector is ready\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "\n",
    "#Testing settings\n",
    "# Generate test file names (similar to training)\n",
    "test_files = list(generate_time_file_names('19_00_00', '20_30_00', folder_path))\n",
    "\n",
    "X_test, y_test = [], []\n",
    "for file_name in test_files:\n",
    "    sat_positions, sat_neighbors, sat_top_3_map, sat_labels = process_file(file_name)\n",
    "    if sat_positions is None:\n",
    "        continue  \n",
    "\n",
    "    for sat in sat_positions.keys():\n",
    "        sat_pos = sat_positions[sat]\n",
    "        neighbor_pos = sat_neighbors[sat]\n",
    "        top_3_preds = sat_top_3_map[sat]\n",
    "        y_label = sat_labels[sat]\n",
    "\n",
    "        feature_vector = np.concatenate([sat_pos] + neighbor_pos + [top_3_preds])\n",
    "        X_test.append(feature_vector)\n",
    "        y_test.append(y_label)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.vstack(y_test)\n",
    "\n",
    "# ✅ Ensure Test Data Also Has 21 Features Before Scaling\n",
    "assert X_test.shape[1] == 21, f\"Test data should have 21 features but has {X_test.shape[1]}!\"\n",
    "X_test = scaler.transform(X_test)\n",
    "print(f\"Final X_test shape before scaling: {X_test.shape}\")  # Debugging\n",
    "assert X_test.shape[1] == X_train.shape[1], \"Feature mismatch! Check preprocessing.\"\n",
    "\n",
    "# Normalize test data using the same scaler as training\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Define Model 2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, LeakyReLU\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "model_2 = Sequential([\n",
    "    Dense(256, kernel_regularizer='l2'),\n",
    "    LeakyReLU(alpha=0.05),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(128, kernel_regularizer='l2'),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(64, kernel_regularizer='l2'),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(1, activation='linear')  # Single output: final gateway index\n",
    "])\n",
    "\n",
    "\n",
    "# Compile model\n",
    "optimizer = RMSprop(learning_rate=0.0005, rho=0.9)\n",
    "model_2.compile(optimizer=optimizer, \n",
    "                loss=CategoricalCrossentropy(label_smoothing=0.1), \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Train Model 2\n",
    "history = model_2.fit(X_train, y_train, epochs=50, batch_size=256, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Evaluate Model\n",
    "train_loss, train_accuracy = model_2.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss, test_accuracy = model_2.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print Results\n",
    "print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Save Model\n",
    "model_2.save('stage_2_neigh_model.h5')\n",
    "print(\"Model training and evaluation complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m top_3_preds \u001b[38;5;241m=\u001b[39m get_top_3_predictions(model_1, \u001b[43mX_input\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_input' is not defined"
     ]
    }
   ],
   "source": [
    "final_gateway_predictions = model_2.predict(X_test)  # Softmax output\n",
    "final_gateways = np.argmax(final_gateway_predictions, axis=1)  # Pick highest probability\n",
    "\n",
    "print(f\"Final Predicted Gateways: {final_gateways}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "train_loss, train_accuracy = model_2.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss, test_accuracy = model_2.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"\\nTraining Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Compute Top-K Accuracy\n",
    "def compute_top_k_accuracy(model, X_test, y_test, k_values=[1, 3, 5]):\n",
    "    predictions = model.predict(X_test)\n",
    "    top_k_indices = {k: np.argsort(predictions, axis=1)[:, -k:] for k in k_values}\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    top_k_accuracies = {k: np.mean(np.any(top_k_indices[k] == y_true[:, None], axis=1)) * 100 for k in k_values}\n",
    "    return top_k_accuracies\n",
    "\n",
    "top_k_accuracies = compute_top_k_accuracy(model, X_test, y_test, k_values=[1, 3, 5])\n",
    "print(\"\\nTop-K Accuracy Results:\")\n",
    "for k, acc in top_k_accuracies.items():\n",
    "    print(f\"Top-{k} Accuracy: {acc:.2f}%\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save scaler for Model 1\n",
    "with open('stage_2_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to Plot Model Loss and Accuracy\n",
    "def plot_model_history(history):\n",
    "    # Plot Loss\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Call this function after training the model\n",
    "plot_model_history(history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
