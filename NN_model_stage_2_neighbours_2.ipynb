{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, LeakyReLU\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import ast  \n",
    "\n",
    "# Load Model 1 & Model 2\n",
    "model_1 = load_model('stage_1_model.h5')\n",
    "model_2 = load_model('stage_2_neigh_model.h5')\n",
    "\n",
    "# Load the same scaler used for training\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# File path for logging\n",
    "csv_file = 'satellite_predictions_per_timestep.csv'\n",
    "\n",
    "# If the file does not exist, create it with headers\n",
    "if not os.path.exists(csv_file):\n",
    "    pd.DataFrame(columns=['timestamp', 'satellite', 'predicted_gateways']).to_csv(csv_file, index=False)\n",
    "\n",
    "# Function to generate filenames\n",
    "def generate_time_file_names(start_time_str, end_time_str, folder_path, prefix='optimal_file_data_'):\n",
    "    start_time = datetime.strptime(start_time_str, '%H_%M_%S')\n",
    "    end_time = datetime.strptime(end_time_str, '%H_%M_%S')\n",
    "    current_time = start_time\n",
    "    while current_time <= end_time:\n",
    "        time_str = current_time.strftime('%H_%M_%S')\n",
    "        file_name = f\"{prefix}{time_str}.csv\"\n",
    "        yield os.path.join(folder_path, file_name)\n",
    "        current_time += timedelta(seconds=20)\n",
    "\n",
    "# Function to process each file\n",
    "def process_file(file_name):\n",
    "    if not os.path.exists(file_name):\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    timestamp = file_name.split('_')[-1].replace('.csv', '')  # Extract timestamp\n",
    "\n",
    "    optimal_data = pd.read_csv(file_name, usecols=['feed_sat', 'Latitude', 'Longitude', 'Altitude', 'optimal_gateway_matrix'])\n",
    "    optimal_data.drop_duplicates(subset=['feed_sat'], inplace=True)\n",
    "\n",
    "    if optimal_data.empty:\n",
    "        return None, None, None, None, None  # Skip empty files\n",
    "\n",
    "    optimal_data[['Latitude', 'Longitude', 'Altitude']] = optimal_data[['Latitude', 'Longitude', 'Altitude']].astype(float)\n",
    "\n",
    "    sat_positions = {feed_sat: np.array([lat, lon, alt], dtype=np.float32) \n",
    "                     for feed_sat, lat, lon, alt in zip(optimal_data['feed_sat'], \n",
    "                                                        optimal_data['Latitude'], \n",
    "                                                        optimal_data['Longitude'], \n",
    "                                                        optimal_data['Altitude'])}\n",
    "\n",
    "    optimal_data['optimal_gateway_matrix'] = optimal_data['optimal_gateway_matrix'].apply(lambda x: np.array(ast.literal_eval(x), dtype=np.float32))\n",
    "    sat_labels = dict(zip(optimal_data['feed_sat'], optimal_data['optimal_gateway_matrix']))\n",
    "    \n",
    "    sat_list = list(sat_positions.keys())\n",
    "\n",
    "    if not sat_positions:\n",
    "        return None, None, None, None, None  # Skip empty datasets\n",
    "\n",
    "    X_input = np.array(list(sat_positions.values()), dtype=np.float32)\n",
    "\n",
    "    if X_input.shape[1] == 3:\n",
    "        X_input = np.hstack([X_input, np.zeros((X_input.shape[0], 1))])\n",
    "\n",
    "    # Normalize input for Model 1\n",
    "    X_input = scaler.fit_transform(X_input)\n",
    "\n",
    "    top_3_preds = get_top_3_predictions(model_1, X_input)\n",
    "    sat_top_3_map = {sat: pred for sat, pred in zip(sat_list, top_3_preds)}\n",
    "\n",
    "    sat_neighbors = {}\n",
    "    for sat, pos in sat_positions.items():\n",
    "        distances = [(other_sat, np.linalg.norm(pos - sat_positions[other_sat])) \n",
    "                     for other_sat in sat_list if other_sat != sat]\n",
    "        nearest_neighbors = sorted(distances, key=lambda x: x[1])[:4]\n",
    "        sat_neighbors[sat] = [sat_positions[n[0]] for n in nearest_neighbors]\n",
    "\n",
    "    return sat_positions, sat_neighbors, sat_top_3_map, sat_labels, timestamp\n",
    "\n",
    "# Function to get top-3 predictions\n",
    "def get_top_3_predictions(model, X):\n",
    "    predictions = model.predict(X, batch_size=256, verbose=0)\n",
    "    top_3_indices = np.argsort(predictions, axis=1)[:, -3:]\n",
    "    top_3_probs = np.take_along_axis(predictions, top_3_indices, axis=1)\n",
    "    return np.hstack((top_3_indices, top_3_probs))\n",
    "\n",
    "# Training settings\n",
    "folder_path = r\"C:\\Users\\aruna\\Desktop\\MS Thesis\\Real Data\\Files with position\"\n",
    "train_files = list(generate_time_file_names('16_00_00', '19_00_00', folder_path))\n",
    "test_files = list(generate_time_file_names('19_00_00', '20_30_00', folder_path))\n",
    "\n",
    "# Set mode flag to track whether training or testing is being processed\n",
    "processing_mode = None  \n",
    "\n",
    "X_train, y_train = [], []\n",
    "batch_predictions = []  # Store batch results before writing\n",
    "\n",
    "# Process training data\n",
    "print(\"\\nðŸŸ¢ Processing Training Files...\\n\")\n",
    "for i, file_name in enumerate(train_files):\n",
    "    sat_positions, sat_neighbors, sat_top_3_map, sat_labels, timestamp = process_file(file_name)\n",
    "    if sat_positions is None:\n",
    "        continue  \n",
    "\n",
    "    for sat in sat_positions.keys():\n",
    "        sat_pos = sat_positions[sat]\n",
    "        neighbor_pos = sat_neighbors[sat]\n",
    "        top_3_preds = sat_top_3_map[sat]\n",
    "        y_label = sat_labels[sat]\n",
    "\n",
    "        feature_vector = np.concatenate([sat_pos] + neighbor_pos + [top_3_preds])\n",
    "        X_train.append(feature_vector)\n",
    "        y_train.append(y_label)\n",
    "\n",
    "        batch_predictions.append({'timestamp': timestamp, 'satellite': sat, 'predicted_gateways': top_3_preds.tolist()})\n",
    "\n",
    "    if i % 50 == 0 and batch_predictions:\n",
    "        df_predictions = pd.DataFrame(batch_predictions)\n",
    "        df_predictions.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "        batch_predictions = []  \n",
    "\n",
    "if batch_predictions:\n",
    "    df_predictions = pd.DataFrame(batch_predictions)\n",
    "    df_predictions.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.vstack(y_train)\n",
    "\n",
    "# Ensure Scaler is Fitted on 21 Features\n",
    "assert X_train.shape[1] == 21, \"Training data should have 21 features!\"\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "# Process testing data\n",
    "print(\"\\nðŸ”µ Processing Testing Files...\\n\")\n",
    "X_test, y_test = [], []\n",
    "for file_name in test_files:\n",
    "    sat_positions, sat_neighbors, sat_top_3_map, sat_labels, timestamp = process_file(file_name)\n",
    "    if sat_positions is None:\n",
    "        continue  \n",
    "\n",
    "    for sat in sat_positions.keys():\n",
    "        sat_pos = sat_positions[sat]\n",
    "        neighbor_pos = sat_neighbors[sat]\n",
    "        top_3_preds = sat_top_3_map[sat]\n",
    "        y_label = sat_labels[sat]\n",
    "\n",
    "        feature_vector = np.concatenate([sat_pos] + neighbor_pos + [top_3_preds])\n",
    "        X_test.append(feature_vector)\n",
    "        y_test.append(y_label)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.vstack(y_test)\n",
    "\n",
    "# Ensure Test Data Has 21 Features Before Scaling\n",
    "assert X_test.shape[1] == 21, f\"Test data should have 21 features but has {X_test.shape[1]}!\"\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train Model 2\n",
    "history = model_2.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save Model\n",
    "model_2.save('stage_2_neigh_model.h5')\n",
    "\n",
    "print(\"\\nâœ… Model training and satellite predictions saved efficiently!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
