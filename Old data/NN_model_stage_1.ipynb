{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, LeakyReLU\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "\n",
    "# Function to generate filenames based on time intervals\n",
    "def generate_time_file_names(start_time_str, end_time_str, folder_path, prefix='optimal_file_data_'):\n",
    "    start_time = datetime.strptime(start_time_str, '%H_%M_%S')\n",
    "    end_time = datetime.strptime(end_time_str, '%H_%M_%S')\n",
    "    current_time = start_time\n",
    "    while current_time <= end_time:\n",
    "        time_str = current_time.strftime('%H_%M_%S')\n",
    "        file_name = f\"{prefix}{time_str}.csv\"\n",
    "        yield os.path.join(folder_path, file_name)\n",
    "        current_time += timedelta(seconds=20)\n",
    "\n",
    "# Function to load and process data\n",
    "def load_data(file_names):\n",
    "    data_list = []\n",
    "    for file_name in file_names:\n",
    "        if os.path.exists(file_name):\n",
    "            df = pd.read_csv(file_name, usecols=['feed_sat', 'Latitude', 'Longitude', 'Altitude', 'optimal_gateway_matrix'])\n",
    "            df.dropna(inplace=True)\n",
    "            data_list.append(df)\n",
    "    return pd.concat(data_list, ignore_index=True) if data_list else None\n",
    "\n",
    "# Time range settings\n",
    "start_time_train = '16_00_00'\n",
    "end_time_train = '19_00_00'\n",
    "start_time_test = '19_00_00'\n",
    "end_time_test = '20_30_00'\n",
    "folder_path = r\"C:\\Users\\aruna\\Desktop\\MS Thesis\\Real Data\\Files with position\"\n",
    "\n",
    "train_files = list(generate_time_file_names(start_time_train, end_time_train, folder_path))\n",
    "test_files = list(generate_time_file_names(start_time_test, end_time_test, folder_path))\n",
    "\n",
    "train_data = load_data(train_files)\n",
    "test_data = load_data(test_files)\n",
    "\n",
    "# Feature and target extraction\n",
    "X_train = train_data[['feed_sat', 'Latitude', 'Longitude', 'Altitude']].values\n",
    "X_test = test_data[['feed_sat', 'Latitude', 'Longitude', 'Altitude']].values\n",
    "\n",
    "def parse_gateway_matrix(matrix_str):\n",
    "    try:\n",
    "        return np.array(ast.literal_eval(matrix_str))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "train_data['optimal_gateway_matrix'] = train_data['optimal_gateway_matrix'].apply(parse_gateway_matrix)\n",
    "test_data['optimal_gateway_matrix'] = test_data['optimal_gateway_matrix'].apply(parse_gateway_matrix)\n",
    "train_data.dropna(subset=['optimal_gateway_matrix'], inplace=True)\n",
    "test_data.dropna(subset=['optimal_gateway_matrix'], inplace=True)\n",
    "\n",
    "y_train = np.vstack(train_data['optimal_gateway_matrix'].values)\n",
    "y_test = np.vstack(test_data['optimal_gateway_matrix'].values)\n",
    "num_gateways = y_train.shape[1]\n",
    "\n",
    "# Normalization\n",
    "scaler = MinMaxScaler()\n",
    "X_train[:, :] = scaler.fit_transform(X_train)\n",
    "X_test[:, :] = scaler.transform(X_test)\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 5:  # Reduce LR sooner\n",
    "        return lr\n",
    "    elif epoch < 10:\n",
    "        return lr * 0.5\n",
    "    else:\n",
    "        return lr * 0.1\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential([\n",
    "    Dense(128, kernel_regularizer='l2'),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.6),\n",
    "\n",
    "    Dense(128, kernel_regularizer='l2'),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(64, kernel_regularizer='l2'),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(num_gateways, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "optimizer = RMSprop(learning_rate=0.001, rho=0.9)\n",
    "model.compile(optimizer=optimizer, loss=CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7)\n",
    "\n",
    "# Training\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.2,  \n",
    "                    epochs=75,  \n",
    "                    batch_size=256,  \n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stopping, lr_reducer, lr_scheduler])\n",
    "\n",
    "# Evaluation\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"\\nTraining Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Compute Top-K Accuracy\n",
    "def compute_top_k_accuracy(model, X_test, y_test, k_values=[1, 3, 5]):\n",
    "    predictions = model.predict(X_test)\n",
    "    top_k_indices = {k: np.argsort(predictions, axis=1)[:, -k:] for k in k_values}\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    top_k_accuracies = {k: np.mean(np.any(top_k_indices[k] == y_true[:, None], axis=1)) * 100 for k in k_values}\n",
    "    return top_k_accuracies\n",
    "\n",
    "top_k_accuracies = compute_top_k_accuracy(model, X_test, y_test, k_values=[1, 3, 5])\n",
    "print(\"\\nTop-K Accuracy Results:\")\n",
    "for k, acc in top_k_accuracies.items():\n",
    "    print(f\"Top-{k} Accuracy: {acc:.2f}%\")\n",
    "\n",
    "# Save model\n",
    "model.save('NN_model_1.h5')\n",
    "print(\"\\nModel training and evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import ast\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load Model\n",
    "model = load_model('NN_model_2.h5')  # Ensure correct model path\n",
    "scaler = joblib.load('scaler_2.pkl')  # Ensure the same scaler used for training\n",
    "\n",
    "# File Path\n",
    "file_path = r\"C:\\Users\\aruna\\Desktop\\MS Thesis\\Real Data\\Files with position\\optimal_file_data_00_01_20.csv\"\n",
    "# Load File\n",
    "df = pd.read_csv(file_path, usecols=['feed_sat', 'Latitude', 'Longitude', 'Altitude', 'optimal_gateway_matrix', 'gw'])\n",
    "\n",
    "# Function to Parse Gateway Matrix\n",
    "def parse_gateway_matrix(matrix_str):\n",
    "    try:\n",
    "        return np.array(ast.literal_eval(matrix_str))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df['optimal_gateway_matrix'] = df['optimal_gateway_matrix'].apply(parse_gateway_matrix)\n",
    "df.dropna(subset=['optimal_gateway_matrix'], inplace=True)\n",
    "\n",
    "# Extract Features (feed_sat, Latitude, Longitude, Altitude)\n",
    "X_test = df[['feed_sat', 'Latitude', 'Longitude', 'Altitude']].values\n",
    "\n",
    "# Normalize Features\n",
    "X_test[:, :] = scaler.transform(X_test)\n",
    "\n",
    "# Extract Actual Gateway\n",
    "actual_gateway = df['gw'].values\n",
    "# Get Predictions from Model\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Get Top 3 Gateway Predictions\n",
    "top_3_predictions = np.argsort(predictions, axis=1)[:, -3:][:, ::-1]  # Get top 3 indices sorted by confidence\n",
    "\n",
    "# Get Top 1 Gateway Prediction\n",
    "top_1_predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Combine into a DataFrame\n",
    "output_df = pd.DataFrame({\n",
    "    'Satellite': df['feed_sat'].values,\n",
    "    'Predicted Gateway': top_1_predictions,\n",
    "    'Predicted 3 Gateways': [list(row) for row in top_3_predictions],\n",
    "    'Actual Gateway': actual_gateway\n",
    "})\n",
    "\n",
    "# Display Output\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Model Predictions\", dataframe=output_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
