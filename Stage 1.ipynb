{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import pickle\n",
    "\n",
    "# ================================\n",
    "# File Utilities\n",
    "# ================================\n",
    "def generate_time_file_names(start_time_str, end_time_str, folder_path, prefix='optimal_file_data_'):\n",
    "    start_time = datetime.strptime(start_time_str, '%H_%M_%S')\n",
    "    end_time = datetime.strptime(end_time_str, '%H_%M_%S')\n",
    "    current_time = start_time\n",
    "    while current_time <= end_time:\n",
    "        time_str = current_time.strftime('%H_%M_%S')\n",
    "        file_name = f\"{prefix}{time_str}.csv\"\n",
    "        yield os.path.join(folder_path, file_name)\n",
    "        current_time += timedelta(seconds=20)\n",
    "\n",
    "def load_data(file_names):\n",
    "    data_list = []\n",
    "    for file_name in file_names:\n",
    "        if os.path.exists(file_name):\n",
    "            df = pd.read_csv(file_name, usecols=[\n",
    "                'feed_sat', 'Latitude', 'Longitude', 'Altitude',\n",
    "                'visible_gateway_matrix', 'optimal_gateway_matrix'\n",
    "            ])\n",
    "            df.drop_duplicates(subset=['feed_sat'], inplace=True)\n",
    "            df.dropna(inplace=True)\n",
    "            data_list.append(df)\n",
    "    return pd.concat(data_list, ignore_index=True) if data_list else None\n",
    "\n",
    "def parse_matrix(matrix_str):\n",
    "    try:\n",
    "        return np.array(ast.literal_eval(matrix_str), dtype=np.float32)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# ================================\n",
    "# Load Files\n",
    "# ================================\n",
    "folder_path = r\"C:\\Users\\aruna\\Desktop\\MS Thesis\\Real Data\\Testing with Cell data\"\n",
    "# Grab all CSVs in folder\n",
    "all_files = sorted([\n",
    "    os.path.join(folder_path, f)\n",
    "    for f in os.listdir(folder_path)\n",
    "    if f.endswith('.csv')\n",
    "])\n",
    "\n",
    "# Use 50% for training, 50% for testing\n",
    "split_index = len(all_files) // 2\n",
    "train_files = all_files[:split_index]\n",
    "test_files = all_files[split_index:]\n",
    "\n",
    "print(f\"Total files: {len(all_files)}\")\n",
    "print(f\"Training files: {len(train_files)}\")\n",
    "print(f\"Testing files: {len(test_files)}\")\n",
    "\n",
    "train_files = list(generate_time_file_names(start_time_train, end_time_train, folder_path))\n",
    "test_files = list(generate_time_file_names(start_time_test, end_time_test, folder_path))\n",
    "\n",
    "train_data = load_data(train_files)\n",
    "test_data = load_data(test_files)\n",
    "\n",
    "# ================================\n",
    "# Parse Gateway Matrices\n",
    "# ================================\n",
    "train_data['visible_gateway_matrix'] = train_data['visible_gateway_matrix'].apply(parse_matrix)\n",
    "train_data['optimal_gateway_matrix'] = train_data['optimal_gateway_matrix'].apply(parse_matrix)\n",
    "test_data['visible_gateway_matrix'] = test_data['visible_gateway_matrix'].apply(parse_matrix)\n",
    "test_data['optimal_gateway_matrix'] = test_data['optimal_gateway_matrix'].apply(parse_matrix)\n",
    "\n",
    "# Drop rows with parsing issues\n",
    "train_data.dropna(subset=['visible_gateway_matrix', 'optimal_gateway_matrix'], inplace=True)\n",
    "test_data.dropna(subset=['visible_gateway_matrix', 'optimal_gateway_matrix'], inplace=True)\n",
    "\n",
    "# ================================\n",
    "# Feature and Target Extraction\n",
    "# ================================\n",
    "# Combine position + visible gateway matrix\n",
    "X_train_pos = train_data[['Latitude', 'Longitude', 'Altitude']].values\n",
    "X_test_pos = test_data[['Latitude', 'Longitude', 'Altitude']].values\n",
    "\n",
    "X_train_visible = np.vstack(train_data['visible_gateway_matrix'].values)\n",
    "X_test_visible = np.vstack(test_data['visible_gateway_matrix'].values)\n",
    "\n",
    "X_train = np.hstack([X_train_pos, X_train_visible])\n",
    "X_test = np.hstack([X_test_pos, X_test_visible])\n",
    "\n",
    "y_train = np.vstack(train_data['optimal_gateway_matrix'].values)\n",
    "y_test = np.vstack(test_data['optimal_gateway_matrix'].values)\n",
    "\n",
    "num_gateways = y_train.shape[1]\n",
    "\n",
    "# ================================\n",
    "# Normalize Features\n",
    "# ================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"X_train shape: {X_train_scaled.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape:  {X_test_scaled.shape}\")\n",
    "print(f\"y_test shape:  {y_test.shape}\")\n",
    "\n",
    "# ================================\n",
    "# Build Model\n",
    "# ================================\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(input_dim,), kernel_regularizer='l2'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu', kernel_regularizer='l2'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu', kernel_regularizer='l2'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_gateways, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss=CategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# Train Model\n",
    "# ================================\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, lr_reducer],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# Evaluate\n",
    "# ================================\n",
    "train_loss, train_acc = model.evaluate(X_train_scaled, y_train, verbose=0)\n",
    "test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "print(f\"\\nTraining Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Loss:     {test_loss:.4f}, Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# ================================\n",
    "# Top-k Accuracy\n",
    "# ================================\n",
    "def compute_top_k_accuracy(model, X, y, k_values=[1, 3, 5]):\n",
    "    preds = model.predict(X)\n",
    "    y_true = np.argmax(y, axis=1)\n",
    "    top_k = {k: np.argsort(preds, axis=1)[:, -k:] for k in k_values}\n",
    "    return {\n",
    "        k: np.mean([y_true[i] in top_k[k][i] for i in range(len(y_true))]) * 100\n",
    "        for k in k_values\n",
    "    }\n",
    "\n",
    "top_k = compute_top_k_accuracy(model, X_test_scaled, y_test)\n",
    "print(\"\\nTop-K Accuracy Results:\")\n",
    "for k, acc in top_k.items():\n",
    "    print(f\"Top-{k} Accuracy: {acc:.2f}%\")\n",
    "\n",
    "# ================================\n",
    "# Save Model & Scaler\n",
    "# ================================\n",
    "model.save('stage_1_model.h5')\n",
    "with open('stage_1_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"\\nâœ… Model and scaler saved successfully.\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
